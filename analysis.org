* Ch 2 | Analysis
:PROPERTIES:
:header-args: :session sess :results output value :colnames yes
:END:

#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.1f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

:PROPERTIES:
:header-args: :session sess :results output value table :colnames yes
:END:



#+BEGIN_SRC R :results silent
library(tidyverse)
library(sparklyr)
library(broom)

# connect to local spark 
sc <- spark_connect(master = "local")
#+END_SRC
 
** Import 

When using Spark with R, we need to approach importing data differently. When using Spark, the data is imported into Spark, not R. 

When performing analysis over large datasets, the majority of the data should be available in the Spark cluster (generally from Hive tables or by accessing the file system directly).

We could import all the data into spark, in which case we have a one time upfront cost, or we could not import it and then we could incur the cost with every spark operation since it would need to retrieve a subset from the cluster's storage (usually on disk drives).

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars <- copy_to(sc, mtcars, overwrite = T)
#+END_SRC

** Wrangle

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>% summarise_all(mean)
#+END_SRC

#+RESULTS:
|  mpg | cyl |  disp |    hp | drat |  wt | qsec |  vs |  am | gear | carb |
|------+-----+-------+-------+------+-----+------+-----+-----+------+------|
| 20.1 | 6.2 | 230.7 | 146.7 |  3.6 | 3.2 | 17.8 | 0.4 | 0.4 |  3.7 |  2.8 |

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
mtcars %>% head()
#+END_SRC

#+RESULTS:
|  mpg | cyl |  disp |    hp | drat |  wt | qsec |  vs |  am | gear | carb |
|------+-----+-------+-------+------+-----+------+-----+-----+------+------|
| 21.0 | 6.0 | 160.0 | 110.0 |  3.9 | 2.6 | 16.5 | 0.0 | 1.0 |  4.0 |  4.0 |
| 21.0 | 6.0 | 160.0 | 110.0 |  3.9 | 2.9 | 17.0 | 0.0 | 1.0 |  4.0 |  4.0 |
| 22.8 | 4.0 | 108.0 |  93.0 |  3.9 | 2.3 | 18.6 | 1.0 | 1.0 |  4.0 |  1.0 |
| 21.4 | 6.0 | 258.0 | 110.0 |  3.1 | 3.2 | 19.4 | 1.0 | 0.0 |  3.0 |  1.0 |
| 18.7 | 8.0 | 360.0 | 175.0 |  3.1 | 3.4 | 17.0 | 0.0 | 0.0 |  3.0 |  2.0 |
| 18.1 | 6.0 | 225.0 | 105.0 |  2.8 | 3.5 | 20.2 | 1.0 | 0.0 |  3.0 |  1.0 |

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    summarize_all(mean) %>%
    show_query()
#+END_SRC

#+RESULTS:
|  mpg | cyl |  disp |    hp | drat |  wt | qsec |  vs |  am | gear | carb |
|------+-----+-------+-------+------+-----+------+-----+-----+------+------|
| 20.1 | 6.2 | 230.7 | 146.7 |  3.6 | 3.2 | 17.8 | 0.4 | 0.4 |  3.7 |  2.8 |

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    mutate(transmission = ifelse(am == 0,
                                 "automatic",
                                 "manual")) %>%
    group_by(transmission) %>%
    summarize_all(mean)
#+END_SRC

#+RESULTS: 
| transmission |  mpg | cyl |  disp |    hp | drat |  wt | qsec |  vs |  am | gear | carb |
|--------------+------+-----+-------+-------+------+-----+------+-----+-----+------+------|
| manual       | 24.4 | 5.1 | 143.5 | 126.8 |  4.0 | 2.4 | 17.4 | 0.5 | 1.0 |  4.4 |  2.9 |
| automatic    | 17.1 | 6.9 | 290.4 | 160.3 |  3.3 | 3.8 | 18.2 | 0.4 | 0.0 |  3.2 |  2.7 |

Sometimes we need to perform an operation not yet available through dplyr and sparklyr. Instead of downloading the data into R, there is usually a Hive function within Spark to accomplish what we need.

This next section covers this scenario.

** Built-in Functions 

Spark SQL is based on Hive's SQL conventions. This means we can use any Spark SQL functions to accomplish operations that might not be available in dplyr. dplyr passes functions it does not recognize to the query engine. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    summarise(mpg_percentile = percentile(mpg, 0.25))
#+END_SRC

#+RESULTS:
| mpg_percentile |
|----------------|
|           15.4 |

There is no `percentile` function in R, so dplyr passes it as is to the resulting SQL query 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    summarise(mpg_percentile = percentile(mpg, 0.25)) %>%
    show_query()
#+END_SRC

<SQL>
SELECT percentile(`mpg`, 0.25) AS `mpg_percentile`
FROM `mtcars`

To pass multiple values to percentile, we can call another hive function called array. The output from spark is an array variable which is imported into R as a list variable column. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>% summarise(mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75)))
#+END_SRC

# Source: spark<?> [?? x 1]
  mpg_percentile
  <list>        
1 <list [3]>    

To unlist this, we can use `explode`

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    summarise(mpg_percentile = percentile(mpg, array(0.25, 0.5, 0.75))) %>%
    mutate(mpg_percentile = explode(mpg_percentile))
#+END_SRC

#+RESULTS:
| mpg_percentile |
|----------------|
|           15.4 |
|           19.2 |
|           22.8 |

** Correlations

Spark provides functions to calculate correlations across the entire dataset and return the results to R as a dataframe object 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
ml_corr(cars)
#+END_SRC

#+RESULTS:
|  mpg |  cyl | disp |   hp | drat |   wt | qsec |   vs |   am | gear | carb |
|------+------+------+------+------+------+------+------+------+------+------|
|  1.0 | -0.9 | -0.8 | -0.8 |  0.7 | -0.9 |  0.4 |  0.7 |  0.6 |  0.5 | -0.6 |
| -0.9 |  1.0 |  0.9 |  0.8 | -0.7 |  0.8 | -0.6 | -0.8 | -0.5 | -0.5 |  0.5 |
| -0.8 |  0.9 |  1.0 |  0.8 | -0.7 |  0.9 | -0.4 | -0.7 | -0.6 | -0.6 |  0.4 |
| -0.8 |  0.8 |  0.8 |  1.0 | -0.4 |  0.7 | -0.7 | -0.7 | -0.2 | -0.1 |  0.7 |
|  0.7 | -0.7 | -0.7 | -0.4 |  1.0 | -0.7 |  0.1 |  0.4 |  0.7 |  0.7 | -0.1 |
| -0.9 |  0.8 |  0.9 |  0.7 | -0.7 |  1.0 | -0.2 | -0.6 | -0.7 | -0.6 |  0.4 |
|  0.4 | -0.6 | -0.4 | -0.7 |  0.1 | -0.2 |  1.0 |  0.7 | -0.2 | -0.2 | -0.7 |
|  0.7 | -0.8 | -0.7 | -0.7 |  0.4 | -0.6 |  0.7 |  1.0 |  0.2 |  0.2 | -0.6 |
|  0.6 | -0.5 | -0.6 | -0.2 |  0.7 | -0.7 | -0.2 |  0.2 |  1.0 |  0.8 |  0.1 |
|  0.5 | -0.5 | -0.6 | -0.1 |  0.7 | -0.6 | -0.2 |  0.2 |  0.8 |  1.0 |  0.3 |
| -0.6 |  0.5 |  0.4 |  0.7 | -0.1 |  0.4 | -0.7 | -0.6 |  0.1 |  0.3 |  1.0 |

The `corrr` R package is also able to recognize a spark object and use Spark as the backend. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(corrr)
correlate(cars, use = "pairwise.complete.obs", method = "pearson")
#+END_SRC

#+RESULTS:
| rowname |  mpg |  cyl | disp |   hp | drat |   wt | qsec |   vs |   am | gear | carb |
|---------+------+------+------+------+------+------+------+------+------+------+------|
| mpg     |  nil | -0.9 | -0.8 | -0.8 |  0.7 | -0.9 |  0.4 |  0.7 |  0.6 |  0.5 | -0.6 |
| cyl     | -0.9 |  nil |  0.9 |  0.8 | -0.7 |  0.8 | -0.6 | -0.8 | -0.5 | -0.5 |  0.5 |
| disp    | -0.8 |  0.9 |  nil |  0.8 | -0.7 |  0.9 | -0.4 | -0.7 | -0.6 | -0.6 |  0.4 |
| hp      | -0.8 |  0.8 |  0.8 |  nil | -0.4 |  0.7 | -0.7 | -0.7 | -0.2 | -0.1 |  0.7 |
| drat    |  0.7 | -0.7 | -0.7 | -0.4 |  nil | -0.7 |  0.1 |  0.4 |  0.7 |  0.7 | -0.1 |
| wt      | -0.9 |  0.8 |  0.9 |  0.7 | -0.7 |  nil | -0.2 | -0.6 | -0.7 | -0.6 |  0.4 |
| qsec    |  0.4 | -0.6 | -0.4 | -0.7 |  0.1 | -0.2 |  nil |  0.7 | -0.2 | -0.2 | -0.7 |
| vs      |  0.7 | -0.8 | -0.7 | -0.7 |  0.4 | -0.6 |  0.7 |  nil |  0.2 |  0.2 | -0.6 |
| am      |  0.6 | -0.5 | -0.6 | -0.2 |  0.7 | -0.7 | -0.2 |  0.2 |  nil |  0.8 |  0.1 |
| gear    |  0.5 | -0.5 | -0.6 | -0.1 |  0.7 | -0.6 | -0.2 |  0.2 |  0.8 |  nil |  0.3 |
| carb    | -0.6 |  0.5 |  0.4 |  0.7 | -0.1 |  0.4 | -0.7 | -0.6 |  0.1 |  0.3 |  nil |

We can pipe the results to other corrr functions as well. For example, shave() turns all of the duplicated results into NAs.

#+BEGIN_SRC R :file plot.svg :results graphics file
correlate(cars, use = "pairwise.complete.obs", method = "pearson") %>%
    shave() %>%
    rplot
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

** Visualize 

In general, the heavy liftin of calculating and preparing the data for visualization (such as aggregating data by groups or binning) can be done in Spark and then the much smaller dataset can be collected in R.

In Spark we use the push compute, collect results approach. We want all the data transformations to happen in spark.

#+BEGIN_SRC R :file plot.svg :results graphics file
cars %>%
    group_by(cyl) %>%
    summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
    collect() %>% 
    ggplot(aes(reorder(as.factor(cyl), -mpg), mpg), data = .) +
    geom_col(fill = "mediumpurple", color = "black") +
    xlab("Cylinders") + ylab("MPG") + 
    coord_flip()
#+END_SRC

#+RESULTS:
[[file:plot.svg]]

*** dbplot

 The dbplot package provides helper functions for plotting with remote data. 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(dbplot)

cars
 #+END_SRC

 Scatter plots are generally to go-to for comparing the relationship between two continuous variables. 

 #+BEGIN_SRC R :file plot.svg :results graphics file
mtcars %>%
    ggplot(aes(mpg, wt)) +
    geom_point() +
    theme_minimal()
 #+END_SRC

 However, no amount of pushing the computation to Spark will help here because the data must be plotted in individual dots.

 A good alternative is the rasterplot.

 #+BEGIN_SRC R :file plot.svg :results graphics file
dbplot_raster(mtcars, mpg, wt, resolution = 16) 
 #+END_SRC

 You can also use dbplot to retrieve the raw data and visualize by other means. To retrieve the aggregates and not the plots,

 - db_compute_bins
 - db_compute_count
 - db_compute_raster
 - db_compute_boxplot

** Model

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    ml_linear_regression(mpg ~ .) %>%
    broom::tidy()
#+END_SRC

#+RESULTS:
| term        | estimate | std.error | statistic | p.value |
|-------------+----------+-----------+-----------+---------|
| (Intercept) |     12.3 |      18.7 |       0.7 |     0.5 |
| cyl         |     -0.1 |       1.0 |      -0.1 |     0.9 |
| disp        |      0.0 |       0.0 |       0.7 |     0.5 |
| hp          |     -0.0 |       0.0 |      -1.0 |     0.3 |
| drat        |      0.8 |       1.6 |       0.5 |     0.6 |
| wt          |     -3.7 |       1.9 |      -2.0 |     0.1 |
| qsec        |      0.8 |       0.7 |       1.1 |     0.3 |
| vs          |      0.3 |       2.1 |       0.2 |     0.9 |
| am          |      2.5 |       2.1 |       1.2 |     0.2 |
| gear        |      0.7 |       1.5 |       0.4 |     0.7 |
| carb        |     -0.2 |       0.8 |      -0.2 |     0.8 |

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    ml_generalized_linear_regression(mpg ~ .) %>%
    summary()
#+END_SRC

To make sure the model can be fit as efficiently as possible, you should cache your dataset before fitting it

** Caching

The `compute` command can take the end of a dplyr command and save the results to spark memory

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cars %>%
    mutate(cyl = paste0("cyl_", cyl)) %>%
    compute("cached_cars") -> cached_cars
#+END_SRC

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cached_cars %>%
    ml_linear_regression(mpg ~ .) %>%
    tidy()
#+END_SRC

