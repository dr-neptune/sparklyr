* Pipelines 
:PROPERTIES:
:header-args: :session R-session :results value table :colnames yes
:END:



#+NAME: round-tbl
#+BEGIN_SRC emacs-lisp :var tbl="" fmt="%.2f"
(mapcar (lambda (row)
          (mapcar (lambda (cell)
                    (if (numberp cell)
                        (format fmt cell)
                      cell))
                  row))
        tbl)
#+end_src

#+RESULTS: round-tbl

** Overview 

The building blocks of pipelines are called transformers and estimators, which are collectively referred to as pipeline stages. 

- A transformer can be used to apply transformations to a data frame and return another data frame

- An estimator can be used to create a transformer given some training data. 

For example, a center and scale estimator can learn the mean and standard deviation of some data and store the statistics in a resulting transformer object; this transformer can then be used to normalize the data that it was trained on and also any new, yet unseen, data. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(tidyverse)
library(sparklyr)
library(magrittr)

sc <- spark_connect(master = "local", version = "2.3")
#+END_SRC

#+RESULTS:
: nil

Here is an example of how to define an estimator:

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
(scaler <- ft_standard_scaler(sc,
                             input_col = "features",
                             output_col = "features_scaled",
                             with_mean = TRUE))
#+END_SRC

#+RESULTS:
: nil

We can create some data for which we know the mean and sd and then fit our scaling model to it using the ml_fit function. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
df <- copy_to(sc, data.frame(value = rnorm(100000, mean = 5, sd = 3))) %>%
    # translate the input to a vector column
    ft_vector_assembler(input_cols = "value", output_col = "features")

(scaler_model <- ml_fit(scaler, df))
#+END_SRC

#+RESULTS:
: nil

We can then use the transformer to transform a data frame, using the ml_transform function. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
scaler_model %>%
    ml_transform(df) %>%
    glimpse()
#+END_SRC

** Creation 

A pipeline is simply a sequence of transformers and estimators, and a pipeline model is a pipeline that has been trained on data so all of its components have been converted to transformers. 

We can initialize an empty pipeline with ml_pipeline(sc) and append stages to it:

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
ml_pipeline(sc) %>%
    ft_standard_scaler(input_col = "features",
                       output_col = "features_scaled",
                       with_mean = TRUE)
#+END_SRC

#+RESULTS:
: nil

Alternatively, we can pass stages directly to ml_pipeline:

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
ml_pipeline(scaler) %>% ml_fit(df)
#+END_SRC

#+RESULTS:
: nil

Note: As a result of the design of Spark ML, pipelines are always estimator objects, even if they comprise only transformers. This means that if we have a pipeline with only transformers, we still need to call ml_fit on it to obtain a transformer. 

** Use Cases 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
okc_train <- spark_read_parquet(sc, "data/okc-train_2.parquet")

okc_train %<>%
    select(not_working, age, sex, drinks, drugs, essay1:essay9, essay_length)
#+END_SRC

#+RESULTS:
: nil

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
pipeline <- ml_pipeline(sc) %>%
  ft_string_indexer(input_col = "sex", output_col = "sex_indexed") %>%
  ft_string_indexer(input_col = "drinks", output_col = "drinks_indexed") %>%
  ft_string_indexer(input_col = "drugs", output_col = "drugs_indexed") %>%
  ft_one_hot_encoder_estimator(
    input_cols = c("sex_indexed", "drinks_indexed", "drugs_indexed"),
    output_cols = c("sex_encoded", "drinks_encoded", "drugs_encoded")
  ) %>%
  ft_vector_assembler(
    input_cols = c("age", "sex_encoded", "drinks_encoded", 
                   "drugs_encoded", "essay_length"), 
    output_col = "features"
  ) %>%
  ft_standard_scaler(input_col = "features", output_col = "features_scaled", 
                     with_mean = TRUE) %>%
  ml_logistic_regression(features_col = "features_scaled", 
                         label_col = "not_working")
#+END_SRC

It is worthwhile to try out each of the intermediate steps on a smaller dataframe while prototyping. After an appropriate transformation for the dataset has been found, we can replace the dataframe with ml_pipeline(sc) and it will allow us to apply that pipeline to any dataframe with the appropriate schema. 

** Hyperparameter Tuning 

We can use ml_cross_validator to perform CV. In this example, we test whether centering the variables improves predictions together with various regularization values for logistic regression. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*) 
ml_cross_validator(sc,
                   estimator = pipeline,
                   estimator_param_maps = list(
                       standard_scaler = list(with_mean = c(TRUE, FALSE)),
                       logistic_regression = list(
                           elastic_net_param = c(0.25, 0.75),
                           reg_param = c(.01, .001))),
                   evaluator =
                       ml_binary_classification_evaluator(
                           sc,
                           label_col = "not_working"),
                   num_folds = 10) -> cv
#+END_SRC

In the above cv pipeline we are stating that we wish to 

- try out the standard scaler with both true and false values 
- try out regularization on the logistic regression by trying out the values 0.25 and 0.75 for alpha and 1e-2 and 1e-3 for lambda. 

This will give rise to 2*2*2 = 8 hyperparameter combinations. 

As with any other estimator, we can fit the cross-validator using ml_fit 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
cv_model <- ml_fit(x = cv, dataset = okc_train)
#+END_SRC 

and inspect the results 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
ml_validation_metrics(cv_model) %>%
    arrange(- areaUnderROC)
#+END_SRC

** Operating Modes 

| First Arg             | Returns                 | Example                                       |
|-----------------------+-------------------------+-----------------------------------------------|
| Spark Connection      | Estimator / Transformer | ft_string_indexer(sc)                         |
| Pipeline              | Pipeline                | ml_pipeline(sc) %>% ft_string_indexer()         |
| Dataframe, no formula | data frame              | ft_string_indexer(iris, "Species", "indexed") |
| Dataframe, formula    | sparklyr ML model obj   | ml_logistic_regression(iris, Species ~ .)     |

- If a spark connection is provided, the function returns a transformer or estimator object, which can be utilized directly using ml_fit or ml_transform
- If a pipeline is provided, the function returns a pipeline object with the stage appended to it
- If a dataframe is provided to a feature transformer (those with the prefix ft_), or an ML algorithm without also providing a formula, the function instantiates the pipeline stage object, fits it to the data if necessary (if the stage is an estimator), and then transforms the dataframe returning a dataframe
- If a dataframe and a formula are provided to an ML algorithm that supports the formula interface, sparklyr builds a pipeline model under the hood and returns an ML model object that contains additional metadat information.


** Interoperability 

One of the most powerful aspects of pipelines is that they can be serialized to disk and are fully interoperable with other spark APIs such as python or scala. 

To save a pipeline model, call ml_save and provide a path. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
model_dir <- file.path("spark_model")
ml_save(cv_model$best_model, model_dir, overwrite = TRUE)
#+END_SRC

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
list.dirs(model_dir, full.names = FALSE)
#+END_SRC

#+RESULTS:
| x                                                        |
|----------------------------------------------------------|
|                                                          |
| metadata                                                 |
| stages                                                   |
| stages/0_string_indexer_14387c56ca69                     |
| stages/0_string_indexer_14387c56ca69/data                |
| stages/0_string_indexer_14387c56ca69/metadata            |
| stages/1_string_indexer_14387e507a76                     |
| stages/1_string_indexer_14387e507a76/data                |
| stages/1_string_indexer_14387e507a76/metadata            |
| stages/2_string_indexer_14386add269f                     |
| stages/2_string_indexer_14386add269f/data                |
| stages/2_string_indexer_14386add269f/metadata            |
| stages/3_one_hot_encoder_estimator_143821de6588          |
| stages/3_one_hot_encoder_estimator_143821de6588/data     |
| stages/3_one_hot_encoder_estimator_143821de6588/metadata |
| stages/4_vector_assembler_143877f32184                   |
| stages/4_vector_assembler_143877f32184/metadata          |
| stages/5_standard_scaler_14383e3775c4                    |
| stages/5_standard_scaler_14383e3775c4/data               |
| stages/5_standard_scaler_14383e3775c4/metadata           |
| stages/6_logistic_regression_143829a54f86                |
| stages/6_logistic_regression_143829a54f86/data           |
| stages/6_logistic_regression_143829a54f86/metadata       |

We can dive into a couple of the files to see what type of data was saved. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
spark_read_json(sc,
                file.path(
                    file.path(dir(file.path(model_dir, "stages"),
                                  pattern = "1_string*",
                                  full.names = TRUE),
                              "metadata"))) %>%
    glimpse()
#+END_SRC

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
spark_read_parquet(sc, file.path(
                           file.path(dir(file.path(model_dir, "stages"),
                                         pattern = "6_logistic*",
                                         full.names = TRUE),
                                     "data")))
#+END_SRC

#+RESULTS:
: nil

We can take this information provided by the json and parquet files and recreate the spark model. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
model_reload <- ml_load(sc, model_dir)
#+END_SRC

We can then retrieve the logistic regression stage from this model. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
ml_stage(model_reload, "logistic_regression")
#+END_SRC

Note that the exported JSON and parquet files are agnostic to the API calling them. 

** Deployment 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
spark_disconnect(sc)
#+END_SRC

*** Batch Scoring 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*) :tangle batch_scoring.R
library(sparklyr)

sc <- spark_connect(master = "local", version = "2.3")

spark_model <- ml_load(sc, "spark_model")

#* @post /predict
score_spark <- function(age, sex, drinks,
                        drugs, essay_length) {
    new_data <- data.frame(age = age,
                           sex = sex,
                           drinks = drinks,
                           drugs = drugs,
                           essay_length = essay_length,
                           stringsAsFactors = FALSE)

    new_data_tbl <- copy_to(sc, new_data, overwrite = TRUE)

    ml_transform(spark_model, new_data_tbl) %>%
        dplyr::pull(prediction)
}
#+END_SRC

This file will allow us to call Rscript batch_driver.R from the command line. It will run a background R process serving the plumber api above. 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*) :tangle batch_driver.R
service <- callr::r_bg(function() {
    p <- plumber::plumb("batch_scoring.R")
    p$run(port = 8000)})
#+END_SRC

 #+RESULTS:
 : nil

We can test if the above worked by calling the api

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
httr::content(httr::POST(
                        "http://127.0.0.1:8000/predict",
                        body = '{"age": 42, "sex": "m", "drinks": "not at all", "drugs": "never", "essay_length": 99}'))
#+END_SRC

#+RESULTS:
|  X0L |
|------|
| 0.00 |

We get a return value of 0, meaning this person is likely employed. 

 #+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
service$interrupt()
 #+END_SRC

If we were to measure the time this operation took, it would be on the order of hundreds of milliseconds, which is insufficient for real time. The main bottleneck is the serialization of the R dataframe to a Spark dataframe and back. It also requires an active Spark session, which is a heavy runtime requirement. 

*** Real Time Scoring 

For real time production we want to keep dependencies as light as possible so we can target more platforms for deployment. 

We can use the mleap package, which provides an interface to the MLeap library, to serialize and serve Spark ML models. At runtime the only prerequisites for the environment are the Java Virtual Machine and the MLeap runtime library. This avoids both the spark binaries and expensive overhead in converting data to and from spark dataframes. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
library(sparklyr)
library(mleap)
#+END_SRC

#+RESULTS:
| x         |
|-----------|
| mleap     |
| sparklyr  |
| stats     |
| graphics  |
| grDevices |
| utils     |
| datasets  |
| methods   |
| base      |

mleap must be loaded when spark_connect is called. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
sc <- spark_connect(master = "local",
                    version = "2.3")

spark_model <- ml_load(sc, "spark_model")
#+END_SRC

The way we save a model to MLeap bundle format is very similar to saving a model using the Spark ML pipelines API; the only additional argument is sample_input, which is a spark dataframe with schema that we expect new data to be scored to have. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
sample_input <- data.frame(sex = "m",
                           drinks = "not at all",
                           drugs = "never",
                           essay_length = 99,
                           age = 25,
                           stringsAsFactors = FALSE)

sample_input_tbl <- copy_to(sc, sample_input)

ml_write_bundle(spark_model,
                sample_input = sample_input_tbl,
                "mleap_model.zip",
                overwrite = TRUE)
#+END_SRC

We can now deploy the artifact we just created, mleap_model.zip, in any device that runs Java and has the open source MLeap runtime dependencies, without needing Spark or R. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
spark_disconnect(sc)
#+END_SRC

Before we use this MLeap model, we should make sure the runtime dependencies are installed. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
mleap::install_maven()
mleap::install_mleap()
#+END_SRC

To test this model, we can create a new plumber API to expose it. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*) :tangle mleap_api.R
library(mleap)

mleap_model <- mleap_load_bundle("mleap_model.zip")

#* @post /predict
score_spark <- function(age, sex, drinks, drugs, essay_length) {
    new_data <- data.frame(
        age = as.double(age),
        sex = sex,
        drinks = drinks,
        drugs = drugs,
        essay_length = as.double(essay_length),
        stringsAsFactors = FALSE)

    mleap_transform(mleap_model, new_data)$prediction
}
#+END_SRC


#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*) :tangle batch_driver.R
service <- callr::r_bg(function() {
    p <- plumber::plumb("batch_scoring.R")
    p$run(port = 8000)})
#+END_SRC

We can test if the above worked by calling the api

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
httr::content(httr::POST(
                        "http://127.0.0.1:8000/predict",
                        body = '{"age": 10, "sex": "f", "drinks": "not at all", "drugs": "never", "essay_length": 999900}'))
#+END_SRC

#+RESULTS:
|  X0L |
|------|
| 0.00 |

If we were to time this operation, we would see that the service now returns predictions in tens of milliseconds. 

#+BEGIN_SRC R :post round-tbl[:colnames yes](*this*)
service$interrupt()
#+END_SRC
